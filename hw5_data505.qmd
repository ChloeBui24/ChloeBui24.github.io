---
title: "Classification"
author: "Chloe Bui"
date: "02/24/2025"

format: 
  html:  # You will quite likely want to change all but the last one, to taste
    theme: superhero  
    mainfont: monospace
    highlight-style: github
    title-block-banner: true
    embed-resources: true
---

**Abstract:**

This is a technical blog post of **both** an HTML file *and* [.qmd file](https://raw.githubusercontent.com/cd-public/D505/refs/heads/master/hws/src/cond.qmd) hosted on GitHub pages.

# 1. Setup

**Step Up Code:**

```{r}
sh <- suppressPackageStartupMessages
sh(library(tidyverse))
sh(library(caret))
library(naivebayes)
wine <- readRDS(gzcon(url("https://github.com/cd-public/D505/raw/master/dat/pinot.rds")))
```

# 2. Logistic Concepts

Why do we call it Logistic Regression even though we are using the technique for classification?

We call it Logistic Regression because it models the log-odds of a binary outcome using a regression-like equation, but applies the logistic function to map predictions to probabilities. Despite being used for classification, it retains the name due to its mathematical foundation in regression.


# 3. Modeling

We train a logistic regression algorithm to classify a whether a wine comes from Marlborough using:
1. An 80-20 train-test split.
2. Three features engineered from the description
3. 5-fold cross validation.

We report Kappa after using the model to predict provinces in the holdout sample.

```{r}
library(tidytext)

# extract all words
desc_to_words <- function(df, omits) { 
  df %>%
    unnest_tokens(word, description) %>%
    anti_join(stop_words) %>% # get rid of stop words
    filter(!(word %in% omits))
}
# STEM
words_to_stems <- function(df) { 
  df %>%
    mutate(word = wordStem(word))
}
# word count
filter_by_count <- function(df, j) { 
  df %>%
    count(id, word) %>% 
    group_by(id) %>% mutate(exists = (n>0)) %>% ungroup %>% 
    group_by(word) %>% 
    mutate(total = sum(n)) %>% 
    filter(total > j)
}
# pivot
pivoter <- function(words, df) {
  words %>%
    pivot_wider(id_cols = id, names_from = word, values_from = exists, values_fill = list(exists=0)) %>% 
    right_join(select(df,id,province)) %>% 
    drop_na() %>% 
    select(-id)
}
# Full fucntion
wine_words <- function(df, j, stem) { 

  words <- desc_to_words(df, c("wine","pinot","vineyard"))
  
  if (stem) {
    words <- words_to_stems(words)
  }
  
  words <- filter_by_count(words, j)

  pivoter(words, df)
}
```



```{r}
# split
wino_mar <- wine_words(wine, 200, F) %>% 
           mutate(Marlborough = as.factor(province == "Marlborough")) %>%
           select(-province)
wine_index <- createDataPartition(wino_mar$Marlborough, p = 0.80, list = FALSE)
train <- wino_mar[wine_index, ]
test <- wino_mar[-wine_index, ]

# fit model
control = trainControl(method = "cv", number = 5)
fit_mar <- train( Marlborough~ .,
             data = train, 
             trControl = control,
             method = "glm",
             family = "binomial",
             maxit = 5)

fit_mar
```
```{r}
get_odds <- function(fit) {
  as.data.frame(t(exp(coef(fit$finalModel))))   %>%
  rownames_to_column(var = "name") %>%
  pivot_longer(-name, names_to = "class", values_to = "odds") %>%
  arrange(desc(odds)) %>%
  head()
}
get_odds(fit_mar)
```




# 4. Binary vs Other Classification

What is the difference between determining some form of classification through logistic regression versus methods like $K$-NN and Naive Bayes which performed classifications?

Logistic Regression is a probabilistic model that estimates the likelihood of class membership using a linear decision boundary. It transform linear predictions into probabilities, making it interpretable and useful when understanding feature importance. However, it struggles with non-linear relationships and is sensitive to outliers.

K-NN is a non-parametric method that classifies a data point based on the majority class of its K nearest neighbors. It works well when decision boundaries are complex and non-linear, but its computational cost increases with large datasets, as it requires distance calculations for every prediction. Additionally, K-NN is sensitive to the choice of K, making it less efficient without proper tuning.

Naïve Bayes is a probabilistic model based on Bayes' Theorem. It assumes independence between features, allowing it to efficiently calculate class probabilities. This independence assumption makes Naïve Bayes fast and effective, especially for text classification tasks. However, when features are correlated, its predictions become less reliable, and it performs poorly for complex decision boundaries.

Therefore, Logistic Regression is preferred for interpretable, linearly separable problems, K-NN for non-linear relationships when computational resources allow, and Naïve Bayes for fast, high-dimensional classification tasks, especially with categorical data. 

# 5. ROC Curves

We can display an ROC for the model to explain your model’s quality.

```{r}
library(pROC)
prob <- predict(fit_mar, newdata = test, type = "prob")[,2]
myRoc <- roc(test$Marlborough, prob)
plot(myRoc)
auc(myRoc)
```

